{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DQN Training for Q*bert - Kaggle Notebook\n",
                "\n",
                "This notebook trains 3 DQN variants (Vanilla, Double, Dueling) on Q*bert and logs metrics for comparison.\n",
                "\n",
                "**Algorithms:**\n",
                "- Vanilla DQN (Mnih et al., 2015)\n",
                "- Double DQN (Van Hasselt et al., 2015)\n",
                "- Dueling DQN (Wang et al., 2015)\n",
                "\n",
                "**Expected Performance:**\n",
                "- Vanilla DQN: ~734 avg score\n",
                "- Double DQN: ~1,428 avg score\n",
                "- Dueling DQN: ~2,256 avg score"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q gymnasium[atari]\n",
                "!pip install -q opencv-python\n",
                "!pip install -q tqdm\n",
                "!pip install -q pandas\n",
                "!pip install -q autorom[accept-rom-license]\n",
                "!AutoROM --accept-license"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import gymnasium as gym\n",
                "import ale_py\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "import numpy as np\n",
                "import cv2\n",
                "import random\n",
                "import time\n",
                "import csv\n",
                "import json\n",
                "import os\n",
                "from collections import deque\n",
                "from datetime import datetime\n",
                "from tqdm import tqdm\n",
                "\n",
                "gym.register_envs(ale_py)\n",
                "print(\"✓ Imports complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Hyperparameters\n",
                "FRAME_STACK = 4\n",
                "FRAME_WIDTH = 84\n",
                "FRAME_HEIGHT = 84\n",
                "NUM_ACTIONS = 6\n",
                "\n",
                "LEARNING_RATE = 0.0001\n",
                "GAMMA = 0.99\n",
                "BATCH_SIZE = 32\n",
                "UPDATE_FREQUENCY = 4\n",
                "TARGET_UPDATE = 1000\n",
                "\n",
                "EPSILON_START = 1.0\n",
                "EPSILON_END = 0.01\n",
                "EPSILON_DECAY_STEPS = 500000\n",
                "\n",
                "REPLAY_BUFFER_SIZE = 100000\n",
                "MIN_REPLAY_SIZE = 10000\n",
                "\n",
                "TOTAL_EPISODES = 5000  # Adjust based on Kaggle time limit\n",
                "MAX_STEPS_PER_EPISODE = 10000\n",
                "SAVE_FREQUENCY = 500\n",
                "STEP_LOG_FREQUENCY = 10\n",
                "\n",
                "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"✓ Using device: {DEVICE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Preprocessing Utils"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def preprocess_frame(frame):\n",
                "    \"\"\"Convert 210x160x3 RGB to 84x84x1 grayscale\"\"\"\n",
                "    gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
                "    resized = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_AREA)\n",
                "    normalized = resized / 255.0\n",
                "    return normalized\n",
                "\n",
                "class FrameStack:\n",
                "    \"\"\"Stack last N frames\"\"\"\n",
                "    def __init__(self, num_frames=4):\n",
                "        self.num_frames = num_frames\n",
                "        self.frames = deque(maxlen=num_frames)\n",
                "    \n",
                "    def reset(self, frame):\n",
                "        processed = preprocess_frame(frame)\n",
                "        for _ in range(self.num_frames):\n",
                "            self.frames.append(processed)\n",
                "        return self._get_state()\n",
                "    \n",
                "    def step(self, frame):\n",
                "        processed = preprocess_frame(frame)\n",
                "        self.frames.append(processed)\n",
                "        return self._get_state()\n",
                "    \n",
                "    def _get_state(self):\n",
                "        return np.stack(self.frames, axis=0)\n",
                "\n",
                "print(\"✓ Preprocessing utils defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Replay Buffer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ReplayBuffer:\n",
                "    \"\"\"Circular buffer for experience replay\"\"\"\n",
                "    def __init__(self, capacity=100000):\n",
                "        self.capacity = capacity\n",
                "        self.buffer = []\n",
                "        self.position = 0\n",
                "    \n",
                "    def push(self, state, action, reward, next_state, done):\n",
                "        if len(self.buffer) < self.capacity:\n",
                "            self.buffer.append(None)\n",
                "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
                "        self.position = (self.position + 1) % self.capacity\n",
                "    \n",
                "    def sample(self, batch_size=32):\n",
                "        batch = random.sample(self.buffer, batch_size)\n",
                "        states, actions, rewards, next_states, dones = zip(*batch)\n",
                "        return (\n",
                "            np.array(states),\n",
                "            np.array(actions),\n",
                "            np.array(rewards, dtype=np.float32),\n",
                "            np.array(next_states),\n",
                "            np.array(dones, dtype=np.float32)\n",
                "        )\n",
                "    \n",
                "    def get_reward_stats(self):\n",
                "        if len(self.buffer) == 0:\n",
                "            return {'min': 0, 'max': 0, 'mean': 0, 'std': 0}\n",
                "        rewards = [t[2] for t in self.buffer]\n",
                "        return {\n",
                "            'min': np.min(rewards),\n",
                "            'max': np.max(rewards),\n",
                "            'mean': np.mean(rewards),\n",
                "            'std': np.std(rewards)\n",
                "        }\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.buffer)\n",
                "\n",
                "print(\"✓ Replay buffer defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Logger"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DQNLogger:\n",
                "    \"\"\"Logging system for DQN training\"\"\"\n",
                "    def __init__(self, log_dir, algorithm_name):\n",
                "        self.log_dir = log_dir\n",
                "        self.algorithm_name = algorithm_name\n",
                "        os.makedirs(log_dir, exist_ok=True)\n",
                "        \n",
                "        self.episode_log_path = os.path.join(log_dir, \"training_log.csv\")\n",
                "        self.episode_log_file = open(self.episode_log_path, 'w', newline='')\n",
                "        self.episode_writer = csv.writer(self.episode_log_file)\n",
                "        self.episode_writer.writerow([\n",
                "            'episode', 'total_reward', 'episode_length', 'avg_loss', 'avg_q_value',\n",
                "            'epsilon', 'timestamp', 'total_steps', 'training_time_seconds',\n",
                "            'level_reached', 'buffer_reward_min', 'buffer_reward_max',\n",
                "            'buffer_reward_mean', 'buffer_reward_std'\n",
                "        ])\n",
                "        \n",
                "        self.step_log_path = os.path.join(log_dir, \"step_log.csv\")\n",
                "        self.step_log_file = open(self.step_log_path, 'w', newline='')\n",
                "        self.step_writer = csv.writer(self.step_log_file)\n",
                "        self.step_writer.writerow([\n",
                "            'global_step', 'episode', 'step_in_episode', 'action',\n",
                "            'reward', 'loss', 'q_value', 'epsilon', 'timestamp'\n",
                "        ])\n",
                "        \n",
                "        self.episode_metrics = {'losses': [], 'q_values': [], 'rewards': []}\n",
                "    \n",
                "    def log_step(self, global_step, episode, step_in_episode, action, \n",
                "                 reward, loss, q_value, epsilon):\n",
                "        self.step_writer.writerow([\n",
                "            global_step, episode, step_in_episode, action, reward,\n",
                "            loss if loss is not None else '', q_value, epsilon,\n",
                "            datetime.now().isoformat()\n",
                "        ])\n",
                "        self.episode_metrics['rewards'].append(reward)\n",
                "        if loss is not None:\n",
                "            self.episode_metrics['losses'].append(loss)\n",
                "        if q_value is not None:\n",
                "            self.episode_metrics['q_values'].append(q_value)\n",
                "    \n",
                "    def log_episode(self, episode, total_reward, episode_length, epsilon,\n",
                "                    total_steps, training_time, level_reached=1,\n",
                "                    buffer_reward_stats=None):\n",
                "        avg_loss = sum(self.episode_metrics['losses']) / len(self.episode_metrics['losses']) \\\n",
                "                   if self.episode_metrics['losses'] else 0\n",
                "        avg_q = sum(self.episode_metrics['q_values']) / len(self.episode_metrics['q_values']) \\\n",
                "                if self.episode_metrics['q_values'] else 0\n",
                "        \n",
                "        if buffer_reward_stats is None:\n",
                "            buffer_reward_stats = {'min': 0, 'max': 0, 'mean': 0, 'std': 0}\n",
                "        \n",
                "        self.episode_writer.writerow([\n",
                "            episode, total_reward, episode_length, avg_loss, avg_q, epsilon,\n",
                "            datetime.now().isoformat(), total_steps, training_time, level_reached,\n",
                "            buffer_reward_stats['min'], buffer_reward_stats['max'],\n",
                "            buffer_reward_stats['mean'], buffer_reward_stats['std']\n",
                "        ])\n",
                "        \n",
                "        self.episode_log_file.flush()\n",
                "        self.step_log_file.flush()\n",
                "        self.episode_metrics = {'losses': [], 'q_values': [], 'rewards': []}\n",
                "    \n",
                "    def save_config(self, config_dict):\n",
                "        config_path = os.path.join(self.log_dir, \"config.json\")\n",
                "        with open(config_path, 'w') as f:\n",
                "            json.dump(config_dict, f, indent=2)\n",
                "    \n",
                "    def close(self):\n",
                "        self.episode_log_file.close()\n",
                "        self.step_log_file.close()\n",
                "\n",
                "print(\"✓ Logger defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Neural Network Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class VanillaDQN(nn.Module):\n",
                "    def __init__(self, input_channels=4, num_actions=6):\n",
                "        super(VanillaDQN, self).__init__()\n",
                "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=8, stride=4)\n",
                "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
                "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
                "        self.fc1 = nn.Linear(7 * 7 * 64, 512)\n",
                "        self.fc2 = nn.Linear(512, num_actions)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = F.relu(self.conv1(x))\n",
                "        x = F.relu(self.conv2(x))\n",
                "        x = F.relu(self.conv3(x))\n",
                "        x = x.view(x.size(0), -1)\n",
                "        x = F.relu(self.fc1(x))\n",
                "        return self.fc2(x)\n",
                "\n",
                "class DoubleDQN(nn.Module):\n",
                "    def __init__(self, input_channels=4, num_actions=6):\n",
                "        super(DoubleDQN, self).__init__()\n",
                "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=8, stride=4)\n",
                "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
                "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
                "        self.fc1 = nn.Linear(7 * 7 * 64, 512)\n",
                "        self.fc2 = nn.Linear(512, num_actions)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = F.relu(self.conv1(x))\n",
                "        x = F.relu(self.conv2(x))\n",
                "        x = F.relu(self.conv3(x))\n",
                "        x = x.view(x.size(0), -1)\n",
                "        x = F.relu(self.fc1(x))\n",
                "        return self.fc2(x)\n",
                "\n",
                "class DuelingDQN(nn.Module):\n",
                "    def __init__(self, input_channels=4, num_actions=6):\n",
                "        super(DuelingDQN, self).__init__()\n",
                "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=8, stride=4)\n",
                "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
                "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
                "        \n",
                "        self.value_fc1 = nn.Linear(7 * 7 * 64, 512)\n",
                "        self.value_fc2 = nn.Linear(512, 1)\n",
                "        \n",
                "        self.advantage_fc1 = nn.Linear(7 * 7 * 64, 512)\n",
                "        self.advantage_fc2 = nn.Linear(512, num_actions)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = F.relu(self.conv1(x))\n",
                "        x = F.relu(self.conv2(x))\n",
                "        x = F.relu(self.conv3(x))\n",
                "        x = x.view(x.size(0), -1)\n",
                "        \n",
                "        value = F.relu(self.value_fc1(x))\n",
                "        value = self.value_fc2(value)\n",
                "        \n",
                "        advantage = F.relu(self.advantage_fc1(x))\n",
                "        advantage = self.advantage_fc2(advantage)\n",
                "        \n",
                "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
                "        return q_values\n",
                "\n",
                "print(\"✓ Models defined\")\n",
                "print(f\"  Vanilla/Double DQN: {sum(p.numel() for p in VanillaDQN().parameters()):,} params\")\n",
                "print(f\"  Dueling DQN: {sum(p.numel() for p in DuelingDQN().parameters()):,} params\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. DQN Agents"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class VanillaDQNAgent:\n",
                "    def __init__(self, policy_net, target_net, lr=0.0001, gamma=0.99, device=\"cuda\"):\n",
                "        self.policy_net = policy_net.to(device)\n",
                "        self.target_net = target_net.to(device)\n",
                "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
                "        self.target_net.eval()\n",
                "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
                "        self.gamma = gamma\n",
                "        self.device = device\n",
                "    \n",
                "    def select_action(self, state, epsilon):\n",
                "        if random.random() < epsilon:\n",
                "            return random.randrange(NUM_ACTIONS)\n",
                "        with torch.no_grad():\n",
                "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
                "            q_values = self.policy_net(state_tensor)\n",
                "            return q_values.argmax(1).item()\n",
                "    \n",
                "    def get_max_q_value(self, state):\n",
                "        with torch.no_grad():\n",
                "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
                "            q_values = self.policy_net(state_tensor)\n",
                "            return q_values.max().item()\n",
                "    \n",
                "    def train_step(self, replay_buffer, batch_size=32):\n",
                "        states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
                "        \n",
                "        states = torch.FloatTensor(states).to(self.device)\n",
                "        actions = torch.LongTensor(actions).to(self.device)\n",
                "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
                "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
                "        dones = torch.FloatTensor(dones).to(self.device)\n",
                "        \n",
                "        current_q = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            next_q = self.target_net(next_states).max(1)[0]\n",
                "            target_q = rewards + self.gamma * next_q * (1 - dones)\n",
                "        \n",
                "        loss = F.mse_loss(current_q, target_q)\n",
                "        self.optimizer.zero_grad()\n",
                "        loss.backward()\n",
                "        self.optimizer.step()\n",
                "        return loss.item()\n",
                "    \n",
                "    def update_target_network(self):\n",
                "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
                "\n",
                "class DoubleDQNAgent(VanillaDQNAgent):\n",
                "    def train_step(self, replay_buffer, batch_size=32):\n",
                "        states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
                "        \n",
                "        states = torch.FloatTensor(states).to(self.device)\n",
                "        actions = torch.LongTensor(actions).to(self.device)\n",
                "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
                "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
                "        dones = torch.FloatTensor(dones).to(self.device)\n",
                "        \n",
                "        current_q = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            best_actions = self.policy_net(next_states).argmax(1)\n",
                "            next_q = self.target_net(next_states).gather(1, best_actions.unsqueeze(1)).squeeze(1)\n",
                "            target_q = rewards + self.gamma * next_q * (1 - dones)\n",
                "        \n",
                "        loss = F.mse_loss(current_q, target_q)\n",
                "        self.optimizer.zero_grad()\n",
                "        loss.backward()\n",
                "        self.optimizer.step()\n",
                "        return loss.item()\n",
                "\n",
                "class DuelingDQNAgent(DoubleDQNAgent):\n",
                "    pass\n",
                "\n",
                "print(\"✓ Agents defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Training Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_dqn(algorithm_name, model_class, agent_class, num_episodes=5000):\n",
                "    \"\"\"\n",
                "    Train DQN agent\n",
                "    \n",
                "    Args:\n",
                "        algorithm_name: \"vanilla_dqn\", \"double_dqn\", or \"dueling_dqn\"\n",
                "        model_class: VanillaDQN, DoubleDQN, or DuelingDQN\n",
                "        agent_class: VanillaDQNAgent, DoubleDQNAgent, or DuelingDQNAgent\n",
                "        num_episodes: Number of episodes to train\n",
                "    \"\"\"\n",
                "    log_dir = f\"logs/{algorithm_name}\"\n",
                "    checkpoint_dir = f\"checkpoints/{algorithm_name}\"\n",
                "    os.makedirs(log_dir, exist_ok=True)\n",
                "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
                "    \n",
                "    env = gym.make(\"ALE/Qbert-v5\")\n",
                "    \n",
                "    policy_net = model_class(input_channels=FRAME_STACK, num_actions=NUM_ACTIONS)\n",
                "    target_net = model_class(input_channels=FRAME_STACK, num_actions=NUM_ACTIONS)\n",
                "    agent = agent_class(policy_net, target_net, lr=LEARNING_RATE, gamma=GAMMA, device=DEVICE)\n",
                "    \n",
                "    replay_buffer = ReplayBuffer(capacity=REPLAY_BUFFER_SIZE)\n",
                "    logger = DQNLogger(log_dir, algorithm_name)\n",
                "    logger.save_config({\n",
                "        'algorithm': algorithm_name,\n",
                "        'learning_rate': LEARNING_RATE,\n",
                "        'gamma': GAMMA,\n",
                "        'batch_size': BATCH_SIZE,\n",
                "        'epsilon_start': EPSILON_START,\n",
                "        'epsilon_end': EPSILON_END,\n",
                "        'epsilon_decay_steps': EPSILON_DECAY_STEPS,\n",
                "        'replay_buffer_size': REPLAY_BUFFER_SIZE,\n",
                "        'target_update': TARGET_UPDATE,\n",
                "    })\n",
                "    \n",
                "    global_step = 0\n",
                "    epsilon = EPSILON_START\n",
                "    epsilon_decay = (EPSILON_START - EPSILON_END) / EPSILON_DECAY_STEPS\n",
                "    \n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"Training {algorithm_name.upper()}\")\n",
                "    print(f\"{'='*60}\")\n",
                "    \n",
                "    for episode in tqdm(range(num_episodes), desc=f\"Training {algorithm_name}\"):\n",
                "        episode_start_time = time.time()\n",
                "        \n",
                "        obs, info = env.reset()\n",
                "        frame_stack = FrameStack(num_frames=FRAME_STACK)\n",
                "        state = frame_stack.reset(obs)\n",
                "        \n",
                "        episode_reward = 0\n",
                "        episode_length = 0\n",
                "        level_reached = 1\n",
                "        \n",
                "        for step in range(MAX_STEPS_PER_EPISODE):\n",
                "            action = agent.select_action(state, epsilon)\n",
                "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
                "            done = terminated or truncated\n",
                "            next_state = frame_stack.step(next_obs)\n",
                "            \n",
                "            if 'level' in info:\n",
                "                level_reached = max(level_reached, info['level'])\n",
                "            \n",
                "            replay_buffer.push(state, action, reward, next_state, done)\n",
                "            \n",
                "            loss = None\n",
                "            if len(replay_buffer) > MIN_REPLAY_SIZE and global_step % UPDATE_FREQUENCY == 0:\n",
                "                loss = agent.train_step(replay_buffer, BATCH_SIZE)\n",
                "            \n",
                "            if global_step % TARGET_UPDATE == 0:\n",
                "                agent.update_target_network()\n",
                "            \n",
                "            if global_step % STEP_LOG_FREQUENCY == 0:\n",
                "                q_value = agent.get_max_q_value(state)\n",
                "                logger.log_step(global_step, episode, step, action, reward, loss, q_value, epsilon)\n",
                "            \n",
                "            state = next_state\n",
                "            episode_reward += reward\n",
                "            episode_length += 1\n",
                "            global_step += 1\n",
                "            epsilon = max(EPSILON_END, epsilon - epsilon_decay)\n",
                "            \n",
                "            if done:\n",
                "                break\n",
                "        \n",
                "        episode_time = time.time() - episode_start_time\n",
                "        buffer_stats = replay_buffer.get_reward_stats()\n",
                "        \n",
                "        logger.log_episode(episode, episode_reward, episode_length, epsilon, global_step,\n",
                "                          episode_time, level_reached, buffer_stats)\n",
                "        \n",
                "        if (episode + 1) % SAVE_FREQUENCY == 0:\n",
                "            checkpoint_path = os.path.join(checkpoint_dir, f\"ep_{episode+1}.pth\")\n",
                "            torch.save({\n",
                "                'policy_net_state_dict': policy_net.state_dict(),\n",
                "                'target_net_state_dict': target_net.state_dict(),\n",
                "                'optimizer_state_dict': agent.optimizer.state_dict(),\n",
                "            }, checkpoint_path)\n",
                "        \n",
                "        if (episode + 1) % 100 == 0:\n",
                "            print(f\"\\nEp {episode+1}/{num_episodes} | Reward: {episode_reward:.0f} | \"\n",
                "                  f\"Steps: {episode_length} | Eps: {epsilon:.3f} | Time: {episode_time:.1f}s\")\n",
                "    \n",
                "    final_path = os.path.join(checkpoint_dir, \"final.pth\")\n",
                "    torch.save({\n",
                "        'policy_net_state_dict': policy_net.state_dict(),\n",
                "        'target_net_state_dict': target_net.state_dict(),\n",
                "        'optimizer_state_dict': agent.optimizer.state_dict(),\n",
                "    }, final_path)\n",
                "    \n",
                "    logger.close()\n",
                "    env.close()\n",
                "    \n",
                "    print(f\"\\n✓ {algorithm_name} training complete!\")\n",
                "    print(f\"  Logs: {log_dir}\")\n",
                "    print(f\"  Checkpoints: {checkpoint_dir}\")\n",
                "\n",
                "print(\"✓ Training function defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Train Vanilla DQN"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_dqn(\"vanilla_dqn\", VanillaDQN, VanillaDQNAgent, num_episodes=TOTAL_EPISODES)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Train Double DQN"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_dqn(\"double_dqn\", DoubleDQN, DoubleDQNAgent, num_episodes=TOTAL_EPISODES)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Train Dueling DQN"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_dqn(\"dueling_dqn\", DuelingDQN, DuelingDQNAgent, num_episodes=6000)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 13. Download Logs\n",
                "\n",
                "Download the logs for visualization on your local machine."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Zip logs for download\n",
                "!zip -r dqn_logs.zip logs/\n",
                "!zip -r dqn_checkpoints.zip checkpoints/\n",
                "\n",
                "print(\"✓ Logs and checkpoints zipped\")\n",
                "print(\"  Download: dqn_logs.zip, dqn_checkpoints.zip\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 14. Quick Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "# Load logs\n",
                "vanilla_df = pd.read_csv(\"logs/vanilla_dqn/training_log.csv\")\n",
                "double_df = pd.read_csv(\"logs/double_dqn/training_log.csv\")\n",
                "dueling_df = pd.read_csv(\"logs/dueling_dqn/training_log.csv\")\n",
                "\n",
                "# Print summary\n",
                "print(\"=\"*60)\n",
                "print(\"TRAINING SUMMARY\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "for name, df in [(\"Vanilla DQN\", vanilla_df), (\"Double DQN\", double_df), (\"Dueling DQN\", dueling_df)]:\n",
                "    print(f\"\\n{name}:\")\n",
                "    print(f\"  Average Reward: {df['total_reward'].mean():.2f} ± {df['total_reward'].std():.2f}\")\n",
                "    print(f\"  Max Reward: {df['total_reward'].max():.2f}\")\n",
                "    print(f\"  Average Training Time: {df['training_time_seconds'].mean():.2f}s per episode\")\n",
                "    print(f\"  Total Training Time: {df['training_time_seconds'].sum() / 3600:.2f} hours\")\n",
                "    print(f\"  Max Level Reached: {df['level_reached'].max()}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}