# -*- coding: utf-8 -*-
"""dqn-qbert-training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YWM7qftv2nFaZNJ6IvXxPEdn8lwqLqiU

# DQN Training for Q*bert - Kaggle Notebook (Memory Optimized)

This notebook trains 5 DQN variants (Vanilla, Double, Dueling, Prioritized, Rainbow) on Q*bert with memory optimizations for Kaggle.

**Algorithms:**
- Vanilla DQN (Mnih et al., 2015)
- Double DQN (Van Hasselt et al., 2015)
- Dueling DQN (Wang et al., 2015)
- Prioritized DQN (Schaul et al., 2015) - Prioritized Experience Replay
- Rainbow DQN (Hessel et al., 2017) - Combines 6 improvements

**Memory Optimizations:**
- Reduced replay buffer size (50K)
- Reduced episodes (1000)
- Periodic garbage collection
- Proper tensor cleanup

## 1. Install Dependencies
"""

!pip install -q gymnasium[atari]
!pip install -q opencv-python
!pip install -q tqdm
!pip install -q pandas
!pip install -q autorom[accept-rom-license]
!AutoROM --accept-license

import gymnasium as gym
import ale_py
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import cv2
import random
import time
import csv
import json
import os
import gc
from collections import deque
from datetime import datetime
from tqdm import tqdm

gym.register_envs(ale_py)
print("✓ Imports complete")

"""## 3. Configuration (Memory Optimized)"""

FRAME_STACK = 4
FRAME_WIDTH = 84
FRAME_HEIGHT = 84
NUM_ACTIONS = 6

LEARNING_RATE = 0.0001
GAMMA = 0.99
BATCH_SIZE = 32
UPDATE_FREQUENCY = 4
TARGET_UPDATE = 5000  # Increased for stability

EPSILON_START = 1.0
EPSILON_END = 0.01
EPSILON_DECAY_STEPS = 100000  # Faster decay for exploration to exploitation transition

# MEMORY OPTIMIZED: Reduced to 50K for faster training
REPLAY_BUFFER_SIZE = 50000
MIN_REPLAY_SIZE = 5000  # Also reduce proportionally

# MEMORY OPTIMIZED: Reduced from 5000 to 1000
TOTAL_EPISODES = 3000
MAX_STEPS_PER_EPISODE = 10000
SAVE_FREQUENCY = 250  
STEP_LOG_FREQUENCY = 10
GC_FREQUENCY = 50  

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"✓ Using device: {DEVICE}")
if DEVICE == "cuda":
    print(f"  GPU: {torch.cuda.get_device_name(0)}")
    print(f"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

"""## 4. Preprocessing Utils"""

def preprocess_frame(frame):
    """Convert 210x160x3 RGB to 84x84x1 grayscale, return uint8 to save memory"""
    gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)
    resized = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_AREA)
    # Return uint8 [0-255] instead of float32 [0-1] to save memory
    return resized.astype(np.uint8)

class FrameStack:
    """Stack last N frames"""
    def __init__(self, num_frames=4):
        self.num_frames = num_frames
        self.frames = deque(maxlen=num_frames)

    def reset(self, frame):
        processed = preprocess_frame(frame)
        for _ in range(self.num_frames):
            self.frames.append(processed)
        return self._get_state()

    def step(self, frame):
        processed = preprocess_frame(frame)
        self.frames.append(processed)
        return self._get_state()

    def _get_state(self):
        # Stack frames as uint8, will be converted to float32 in buffer sampling
        return np.stack(self.frames, axis=0).astype(np.uint8)

print("✓ Preprocessing utils defined")

"""## 5. Replay Buffer (Memory Optimized)"""

class ReplayBuffer:
    """Circular buffer for experience replay"""
    def __init__(self, capacity=100000):
        self.capacity = capacity
        self.buffer = []
        self.position = 0

    def push(self, state, action, reward, next_state, done):
        if len(self.buffer) < self.capacity:
            self.buffer.append(None)
        # States already in uint8 from preprocessing, store directly
        self.buffer[self.position] = (
            state,  # Already uint8 [0-255]
            action,
            np.float32(reward),
            next_state,  # Already uint8 [0-255]
            np.float32(done)
        )
        self.position = (self.position + 1) % self.capacity

    def sample(self, batch_size=32):
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        # Return uint8 arrays, normalization will be done on GPU
        return (
            np.array(states, dtype=np.uint8),
            np.array(actions),
            np.array(rewards, dtype=np.float32),
            np.array(next_states, dtype=np.uint8),
            np.array(dones, dtype=np.float32)
        )

    def get_reward_stats(self):
        if len(self.buffer) == 0:
            return {'min': 0, 'max': 0, 'mean': 0, 'std': 0}

        rewards = [t[2] for t in self.buffer]
        return {
            'min': np.min(rewards),
            'max': np.max(rewards),
            'mean': np.mean(rewards),
            'std': np.std(rewards)
        }

    def __len__(self):
        return len(self.buffer)


print("✓ Replay buffer defined")

"""## 6. Logger"""

class DQNLogger:
    """Logging system for DQN training"""
    def __init__(self, log_dir, algorithm_name):
        self.log_dir = log_dir
        self.algorithm_name = algorithm_name
        os.makedirs(log_dir, exist_ok=True)

        self.episode_log_path = os.path.join(log_dir, "training_log.csv")
        self.episode_log_file = open(self.episode_log_path, 'w', newline='', buffering=1)  # Line buffering
        self.episode_writer = csv.writer(self.episode_log_file)
        self.episode_writer.writerow([
            'episode', 'total_reward', 'episode_length', 'avg_loss', 'avg_q_value',
            'epsilon', 'timestamp', 'total_steps', 'training_time_seconds',
            'eval_reward', 'buffer_reward_min', 'buffer_reward_max',
            'buffer_reward_mean', 'buffer_reward_std'
        ])

        self.step_log_path = os.path.join(log_dir, "step_log.csv")
        self.step_log_file = open(self.step_log_path, 'w', newline='', buffering=1)  # Line buffering
        self.step_writer = csv.writer(self.step_log_file)
        self.step_writer.writerow([
            'global_step', 'episode', 'step_in_episode', 'action',
            'reward', 'loss', 'q_value', 'epsilon', 'timestamp'
        ])

        self.episode_metrics = {'losses': [], 'q_values': [], 'rewards': []}

    def log_step(self, global_step, episode, step_in_episode, action,
                 reward, loss, q_value, epsilon):
        self.step_writer.writerow([
            global_step, episode, step_in_episode, action, reward,
            loss if loss is not None else '', q_value, epsilon,
            datetime.now().isoformat()
        ])
        self.episode_metrics['rewards'].append(reward)
        if loss is not None:
            self.episode_metrics['losses'].append(loss)
        if q_value is not None:
            self.episode_metrics['q_values'].append(q_value)

    def log_episode(self, episode, total_reward, episode_length, epsilon,
                    total_steps, training_time,
                    buffer_reward_stats=None, eval_reward=None):
        avg_loss = sum(self.episode_metrics['losses']) / len(self.episode_metrics['losses']) \
                   if self.episode_metrics['losses'] else 0
        avg_q = sum(self.episode_metrics['q_values']) / len(self.episode_metrics['q_values']) \
                if self.episode_metrics['q_values'] else 0

        if buffer_reward_stats is None:
            buffer_reward_stats = {'min': 0, 'max': 0, 'mean': 0, 'std': 0}

        self.episode_writer.writerow([
            episode, total_reward, episode_length, avg_loss, avg_q, epsilon,
            datetime.now().isoformat(), total_steps, training_time,
            eval_reward if eval_reward is not None else '',
            buffer_reward_stats['min'], buffer_reward_stats['max'],
            buffer_reward_stats['mean'], buffer_reward_stats['std']
        ])

        # Files are line-buffered, but explicit flush for safety
        self.episode_log_file.flush()
        self.step_log_file.flush()
        self.episode_metrics = {'losses': [], 'q_values': [], 'rewards': []}

    def save_config(self, config_dict):
        config_path = os.path.join(self.log_dir, "config.json")
        with open(config_path, 'w') as f:
            json.dump(config_dict, f, indent=2)

    def close(self):
        self.episode_log_file.close()
        self.step_log_file.close()

print("✓ Logger defined")

"""## 7. Neural Network Models"""

class VanillaDQN(nn.Module):
    def __init__(self, input_channels=4, num_actions=6):
        super(VanillaDQN, self).__init__()
        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=8, stride=4)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)
        self.fc1 = nn.Linear(7 * 7 * 64, 512)
        self.fc2 = nn.Linear(512, num_actions)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        return self.fc2(x)

class DoubleDQN(nn.Module):
    def __init__(self, input_channels=4, num_actions=6):
        super(DoubleDQN, self).__init__()
        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=8, stride=4)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)
        self.fc1 = nn.Linear(7 * 7 * 64, 512)
        self.fc2 = nn.Linear(512, num_actions)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        return self.fc2(x)

class DuelingDQN(nn.Module):
    def __init__(self, input_channels=4, num_actions=6):
        super(DuelingDQN, self).__init__()
        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=8, stride=4)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)

        self.value_fc1 = nn.Linear(7 * 7 * 64, 512)
        self.value_fc2 = nn.Linear(512, 1)

        self.advantage_fc1 = nn.Linear(7 * 7 * 64, 512)
        self.advantage_fc2 = nn.Linear(512, num_actions)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = x.view(x.size(0), -1)

        value = F.relu(self.value_fc1(x))
        value = self.value_fc2(value)

        advantage = F.relu(self.advantage_fc1(x))
        advantage = self.advantage_fc2(advantage)

        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))
        return q_values

print("✓ Models defined")
print(f"  Vanilla/Double DQN: {sum(p.numel() for p in VanillaDQN().parameters()):,} params")
print(f"  Dueling DQN: {sum(p.numel() for p in DuelingDQN().parameters()):,} params")

"""## 8. DQN Agents (Memory Optimized)"""

class VanillaDQNAgent:
    def __init__(self, policy_net, target_net, config):
        self.policy_net = policy_net
        self.target_net = target_net
        self.config = config

        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()

        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=config.LEARNING_RATE)

        self.device = torch.device(config.DEVICE if torch.cuda.is_available() else "cpu")
        self.policy_net.to(self.device)
        self.target_net.to(self.device)

    def select_action(self, state, epsilon):
        if random.random() < epsilon:
            return random.randrange(self.config.NUM_ACTIONS)
        else:
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device) / 255.0
                q_values = self.policy_net(state_tensor)
                return q_values.argmax(1).item()

    def get_max_q_value(self, state):
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device) / 255.0
            q_values = self.policy_net(state_tensor)
            return q_values.max().item()

    def train_step(self, replay_buffer):
        states, actions, rewards, next_states, dones = replay_buffer.sample(self.config.BATCH_SIZE)

        # Convert uint8 to tensor on GPU, then normalize (faster than CPU normalization)
        states = torch.from_numpy(states).float().to(self.device) / 255.0
        actions = torch.LongTensor(actions).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        next_states = torch.from_numpy(next_states).float().to(self.device) / 255.0
        dones = torch.FloatTensor(dones).to(self.device)

        current_q = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)

        with torch.no_grad():
            next_q = self.target_net(next_states).max(1)[0]
            target_q = rewards + self.config.GAMMA * next_q * (1 - dones)

        # Use Huber loss (smooth_l1_loss) as per DQN Nature 2015 paper
        loss = F.smooth_l1_loss(current_q, target_q)

        self.optimizer.zero_grad()
        loss.backward()
        # Gradient clipping to prevent exploding gradients
        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 10.0)
        self.optimizer.step()

        return loss.item()

    def update_target_network(self):
        self.target_net.load_state_dict(self.policy_net.state_dict())

    def save(self, path):
        torch.save({
            'policy_net_state_dict': self.policy_net.state_dict(),
            'target_net_state_dict': self.target_net.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
        }, path)

    def load(self, path):
        checkpoint = torch.load(path)
        self.policy_net.load_state_dict(checkpoint['policy_net_state_dict'])
        self.target_net.load_state_dict(checkpoint['target_net_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

class DoubleDQNAgent:
    def __init__(self, policy_net, target_net, config):
        self.policy_net = policy_net
        self.target_net = target_net
        self.config = config

        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()

        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=config.LEARNING_RATE)

        self.device = torch.device(config.DEVICE if torch.cuda.is_available() else "cpu")
        self.policy_net.to(self.device)
        self.target_net.to(self.device)

    def select_action(self, state, epsilon):
        if random.random() < epsilon:
            return random.randrange(self.config.NUM_ACTIONS)
        else:
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device) / 255.0
                q_values = self.policy_net(state_tensor)
                return q_values.argmax(1).item()

    def get_max_q_value(self, state):
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device) / 255.0
            q_values = self.policy_net(state_tensor)
            return q_values.max().item()

    def train_step(self, replay_buffer):
        states, actions, rewards, next_states, dones = replay_buffer.sample(self.config.BATCH_SIZE)

        # Convert uint8 to tensor on GPU, then normalize (faster than CPU normalization)
        states = torch.from_numpy(states).float().to(self.device) / 255.0
        actions = torch.LongTensor(actions).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        next_states = torch.from_numpy(next_states).float().to(self.device) / 255.0
        dones = torch.FloatTensor(dones).to(self.device)

        current_q = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)

        # double dqn selection
        with torch.no_grad():
            best_actions = self.policy_net(next_states).argmax(1)
            next_q = self.target_net(next_states).gather(1, best_actions.unsqueeze(1)).squeeze(1)
            target_q = rewards + self.config.GAMMA * next_q * (1 - dones)

        # Use Huber loss (smooth_l1_loss) as per DQN Nature 2015 paper
        loss = F.smooth_l1_loss(current_q, target_q)

        self.optimizer.zero_grad()
        loss.backward()
        # Gradient clipping to prevent exploding gradients
        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 10.0)
        self.optimizer.step()

        return loss.item()

    def update_target_network(self):
        self.target_net.load_state_dict(self.policy_net.state_dict())

    def save(self, path):
        torch.save({
            'policy_net_state_dict': self.policy_net.state_dict(),
            'target_net_state_dict': self.target_net.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
        }, path)

    def load(self, path):
        checkpoint = torch.load(path)
        self.policy_net.load_state_dict(checkpoint['policy_net_state_dict'])
        self.target_net.load_state_dict(checkpoint['target_net_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

class DuelingDQNAgent:
    def __init__(self, policy_net, target_net, config):
        self.policy_net = policy_net
        self.target_net = target_net
        self.config = config

        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()

        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=config.LEARNING_RATE)

        self.device = torch.device(config.DEVICE if torch.cuda.is_available() else "cpu")
        self.policy_net.to(self.device)
        self.target_net.to(self.device)

    def select_action(self, state, epsilon):
        if random.random() < epsilon:
            return random.randrange(self.config.NUM_ACTIONS)
        else:
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device) / 255.0
                q_values = self.policy_net(state_tensor)
                return q_values.argmax(1).item()

    def get_max_q_value(self, state):
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device) / 255.0
            q_values = self.policy_net(state_tensor)
            return q_values.max().item()

    def train_step(self, replay_buffer):
        states, actions, rewards, next_states, dones = replay_buffer.sample(self.config.BATCH_SIZE)

        # Convert uint8 to tensor on GPU, then normalize (faster than CPU normalization)
        states = torch.from_numpy(states).float().to(self.device) / 255.0
        actions = torch.LongTensor(actions).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        next_states = torch.from_numpy(next_states).float().to(self.device) / 255.0
        dones = torch.FloatTensor(dones).to(self.device)

        current_q = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)


        # Dueling DQN uses VANILLA training logic (only architecture differs)
        # NOT Double DQN logic - this is a common mistake
        with torch.no_grad():
            next_q = self.target_net(next_states).max(1)[0]
            target_q = rewards + self.config.GAMMA * next_q * (1 - dones)

        # Use Huber loss (smooth_l1_loss) as per DQN Nature 2015 paper
        loss = F.smooth_l1_loss(current_q, target_q)

        self.optimizer.zero_grad()
        loss.backward()
        # Gradient clipping to prevent exploding gradients
        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 10.0)
        self.optimizer.step()

        return loss.item()

    def update_target_network(self):
        self.target_net.load_state_dict(self.policy_net.state_dict())

    def save(self, path):
        torch.save({
            'policy_net_state_dict': self.policy_net.state_dict(),
            'target_net_state_dict': self.target_net.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
        }, path)

    def load(self, path):
        checkpoint = torch.load(path)
        self.policy_net.load_state_dict(checkpoint['policy_net_state_dict'])
        self.target_net.load_state_dict(checkpoint['target_net_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

print("✓ Agents defined")

"""## 8b. Advanced Components - Prioritized & Rainbow DQN"""

# Prioritized Replay Buffer with Sum Tree
class SumTree:
    """Binary tree for O(log n) priority sampling"""
    def __init__(self, capacity):
        self.capacity = capacity
        self.tree = np.zeros(2 * capacity - 1)
        self.data = [None] * capacity  # Correctly initialize with None
        self.n_entries = 0
        self.write = 0
    
    def _propagate(self, idx, change):
        parent = (idx - 1) // 2
        self.tree[parent] += change
        if parent != 0:
            self._propagate(parent, change)
    
    def _retrieve(self, idx, s):
        left = 2 * idx + 1
        right = left + 1
        
        if left >= len(self.tree):
            return idx
        
        if s <= self.tree[left]:
            return self._retrieve(left, s)
        else:
            return self._retrieve(right, s - self.tree[left])
    
    def total(self):
        return self.tree[0]
    
    def add(self, priority, data):
        idx = self.write + self.capacity - 1
        self.data[self.write] = data
        self.update(idx, priority)
        self.write = (self.write + 1) % self.capacity
        if self.n_entries < self.capacity:
            self.n_entries += 1
    
    def update(self, idx, priority):
        change = priority - self.tree[idx]
        self.tree[idx] = priority
        self._propagate(idx, change)
    
    def get(self, s):
        idx = self._retrieve(0, s)
        data_idx = idx - self.capacity + 1
        return (idx, self.tree[idx], self.data[data_idx])

class PrioritizedReplayBuffer:
    """Prioritized Experience Replay (Schaul et al. 2015)"""
    def __init__(self, capacity=100000, alpha=0.6, beta_start=0.4, beta_frames=100000):
        self.tree = SumTree(capacity)
        self.capacity = capacity
        self.alpha = alpha
        self.beta_start = beta_start
        self.beta_frames = beta_frames
        self.frame = 1
        self.max_priority = 1.0
        self.epsilon = 0.01
    
    def push(self, state, action, reward, next_state, done):
        # States already in uint8 from preprocessing, store directly
        data = (
            state,  # Already uint8 [0-255]
            action, 
            np.float32(reward), 
            next_state,  # Already uint8 [0-255]
            np.float32(done)
        )
        self.tree.add(self.max_priority, data)
    
    def sample(self, batch_size=32):
        batch = []
        indices = []
        priorities = []
        
        total_p = self.tree.total()
        if total_p == 0:
            # Fallback to random if no priorities set (shouldn't happen with MIN_REPLAY_SIZE)
            total_p = 1.0
            
        beta = min(1.0, self.beta_start + self.frame * (1.0 - self.beta_start) / self.beta_frames)
        segment = total_p / batch_size
        
        for i in range(batch_size):
            a = segment * i
            b = segment * (i + 1)
            s = random.uniform(a, b)
            idx, priority, data = self.tree.get(s)
            
            if data is not None:
                batch.append(data)
                indices.append(idx)
                priorities.append(priority)
        
        # If sampling failed to fill the batch (numerical issues), fill with random entries
        while len(batch) < batch_size:
            idx, priority, data = self.tree.get(random.uniform(0, total_p))
            if data is not None:
                batch.append(data)
                indices.append(idx)
                priorities.append(priority)
            else:
                # Absolute fallback if somehow we still get None
                # This ensures batch is NEVER empty if n_entries > 0
                temp_idx = random.randint(0, self.tree.n_entries - 1)
                batch.append(self.tree.data[temp_idx])
                indices.append(temp_idx + self.capacity - 1)
                priorities.append(self.max_priority)

        sampling_probabilities = np.array(priorities) / (self.tree.total() + 1e-8)
        weights = np.power(self.tree.n_entries * sampling_probabilities, -beta)
        weights /= (weights.max() + 1e-8)
        
        states, actions, rewards, next_states, dones = zip(*batch)
        self.frame += 1
        
        # Return uint8 arrays, normalization will be done on GPU
        return (
            np.array(states, dtype=np.uint8),
            np.array(actions), 
            np.array(rewards, dtype=np.float32),
            np.array(next_states, dtype=np.uint8),
            np.array(dones, dtype=np.float32),
            indices, 
            np.array(weights, dtype=np.float32)
        )
    
    def update_priorities(self, indices, errors):
        for idx, error in zip(indices, errors):
            priority = (np.abs(error) + self.epsilon) ** self.alpha
            self.tree.update(idx, priority)
            self.max_priority = max(self.max_priority, priority)
    
    def get_reward_stats(self):
        """Get statistics of rewards in the buffer"""
        if len(self) == 0:
            return {'min': 0, 'max': 0, 'mean': 0, 'std': 0}
        
        # Extract rewards from tree data
        rewards = []
        for i in range(self.tree.capacity):
            if self.tree.data[i] is not None:
                rewards.append(self.tree.data[i][2])  # reward is at index 2
        
        if len(rewards) == 0:
            return {'min': 0, 'max': 0, 'mean': 0, 'std': 0}
        
        return {
            'min': np.min(rewards),
            'max': np.max(rewards),
            'mean': np.mean(rewards),
            'std': np.std(rewards)
        }
    
    def __len__(self):
        return self.tree.n_entries

# Multi-step Buffer for n-step returns
class MultiStepBuffer:
    """Buffer for computing n-step returns"""
    def __init__(self, n_steps=3, gamma=0.99):
        self.n_steps = n_steps
        self.gamma = gamma
        self.buffer = deque(maxlen=n_steps)
    
    def append(self, state, action, reward, next_state, done):
        # CRITICAL: Copy arrays to avoid memory leak from holding references
        self.buffer.append((state.copy(), action, reward, next_state.copy(), done))
    
    def get(self, force=False):
        if not force and len(self.buffer) < self.n_steps:
            return None
        if len(self.buffer) == 0:
            return None
        
        # Calculate n-step return: R = r1 + γ*r2 + γ^2*r3...
        n_step_reward = 0
        for i, transition in enumerate(self.buffer):
            n_step_reward += (self.gamma ** i) * transition[2]
        
        # CRITICAL: Return COPIES to prevent memory leak
        # If we return references, both buffer and replay will hold same arrays
        state_0 = self.buffer[0][0].copy()
        action_0 = self.buffer[0][1]
        next_state_n = self.buffer[-1][3].copy()
        done_n = self.buffer[-1][4]
        
        return state_0, action_0, n_step_reward, next_state_n, done_n
    
    def reset(self):
        self.buffer.clear()
        # Force garbage collection to free memory
        import gc
        gc.collect()
    
    def __len__(self):
        return len(self.buffer)

# Noisy Networks for exploration
class NoisyLinear(nn.Module):
    """Noisy Linear layer (Fortunato et al. 2017)"""
    def __init__(self, in_features, out_features, sigma_init=0.5):
        super(NoisyLinear, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        
        self.weight_mu = nn.Parameter(torch.FloatTensor(out_features, in_features))
        self.weight_sigma = nn.Parameter(torch.FloatTensor(out_features, in_features))
        self.register_buffer('weight_epsilon', torch.FloatTensor(out_features, in_features))
        
        self.bias_mu = nn.Parameter(torch.FloatTensor(out_features))
        self.bias_sigma = nn.Parameter(torch.FloatTensor(out_features))
        self.register_buffer('bias_epsilon', torch.FloatTensor(out_features))
        
        # Cache noise tensors to avoid CPU→GPU copies
        self.register_buffer('epsilon_in_cache', torch.zeros(in_features))
        self.register_buffer('epsilon_out_cache', torch.zeros(out_features))
        
        self.reset_parameters()
        self.reset_noise()
    
    def reset_parameters(self):
        import math
        mu_range = 1 / math.sqrt(self.in_features)
        self.weight_mu.data.uniform_(-mu_range, mu_range)
        self.weight_sigma.data.fill_(0.5 / math.sqrt(self.in_features))
        self.bias_mu.data.uniform_(-mu_range, mu_range)
        self.bias_sigma.data.fill_(0.5 / math.sqrt(self.out_features))
    
    def reset_noise(self):
        # Generate noise directly on device
        self.epsilon_in_cache.normal_()
        self.epsilon_out_cache.normal_()
        
        # Scale noise using factorized Gaussian
        epsilon_in = self.epsilon_in_cache.sign() * self.epsilon_in_cache.abs().sqrt()
        epsilon_out = self.epsilon_out_cache.sign() * self.epsilon_out_cache.abs().sqrt()
        
        self.weight_epsilon.copy_(epsilon_out.outer(epsilon_in))
        self.bias_epsilon.copy_(epsilon_out)
    
    def forward(self, x):
        if self.training:
            weight = self.weight_mu + self.weight_sigma * self.weight_epsilon
            bias = self.bias_mu + self.bias_sigma * self.bias_epsilon
        else:
            weight = self.weight_mu
            bias = self.bias_mu
        return F.linear(x, weight, bias)

# Rainbow DQN Model
class RainbowDQN(nn.Module):
    """Rainbow DQN with Dueling + Noisy Nets + C51"""
    def __init__(self, input_channels=4, num_actions=6, num_atoms=51, v_min=-10, v_max=10):
        super(RainbowDQN, self).__init__()
        self.num_actions = num_actions
        self.num_atoms = num_atoms
        self.v_min = v_min
        self.v_max = v_max
        
        self.register_buffer('support', torch.linspace(v_min, v_max, num_atoms))
        
        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=8, stride=4)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)
        
        self.value_fc = NoisyLinear(7 * 7 * 64, 512)
        self.value_out = NoisyLinear(512, num_atoms)
        
        self.advantage_fc = NoisyLinear(7 * 7 * 64, 512)
        self.advantage_out = NoisyLinear(512, num_actions * num_atoms)
    
    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = x.view(x.size(0), -1)
        
        value = F.relu(self.value_fc(x))
        value = self.value_out(value).view(-1, 1, self.num_atoms)
        
        advantage = F.relu(self.advantage_fc(x))
        advantage = self.advantage_out(advantage).view(-1, self.num_actions, self.num_atoms)
        
        q_atoms = value + advantage - advantage.mean(dim=1, keepdim=True)
        return F.log_softmax(q_atoms, dim=2)
    
    def reset_noise(self):
        self.value_fc.reset_noise()
        self.value_out.reset_noise()
        self.advantage_fc.reset_noise()
        self.advantage_out.reset_noise()
    
    def get_q_values(self, x):
        log_probs = self.forward(x)
        probs = log_probs.exp()
        q_values = (probs * self.support.view(1, 1, -1)).sum(dim=2)
        return q_values

# Prioritized DQN Agent
class PrioritizedDQNAgent:
    """Prioritized DQN Agent"""
    def __init__(self, policy_net, target_net, config):
        self.policy_net = policy_net
        self.target_net = target_net
        self.config = config
        
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()
        
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=config.LEARNING_RATE)
        self.device = torch.device(config.DEVICE if torch.cuda.is_available() else "cpu")
        self.policy_net.to(self.device)
        self.target_net.to(self.device)
    
    def select_action(self, state, epsilon):
        if random.random() < epsilon:
            return random.randrange(self.config.NUM_ACTIONS)
        else:
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device) / 255.0
                q_values = self.policy_net(state_tensor)
                return q_values.argmax(1).item()
    
    def get_max_q_value(self, state):
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device) / 255.0
            q_values = self.policy_net(state_tensor)
            return q_values.max().item()
    
    def train_step(self, replay_buffer):
        states, actions, rewards, next_states, dones, indices, weights = \
            replay_buffer.sample(self.config.BATCH_SIZE)
        
        # Convert uint8 to tensor on GPU, then normalize (faster than CPU normalization)
        states = torch.from_numpy(states).float().to(self.device) / 255.0
        actions = torch.LongTensor(actions).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        next_states = torch.from_numpy(next_states).float().to(self.device) / 255.0
        dones = torch.FloatTensor(dones).to(self.device)
        weights = torch.FloatTensor(weights).to(self.device)
        
        current_q = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)
        
        with torch.no_grad():
            next_q = self.target_net(next_states).max(1)[0]
            target_q = rewards + self.config.GAMMA * next_q * (1 - dones)
        
        td_errors = torch.abs(current_q - target_q).detach().cpu().numpy()
        elementwise_loss = F.smooth_l1_loss(current_q, target_q, reduction='none')
        loss = (elementwise_loss * weights).mean()
        
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 10.0)
        self.optimizer.step()
        
        replay_buffer.update_priorities(indices, td_errors)
        return loss.item()
    
    def update_target_network(self):
        self.target_net.load_state_dict(self.policy_net.state_dict())
    
    def save(self, path):
        torch.save({
            'policy_net_state_dict': self.policy_net.state_dict(),
            'target_net_state_dict': self.target_net.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
        }, path)
    
    def load(self, path):
        checkpoint = torch.load(path)
        self.policy_net.load_state_dict(checkpoint['policy_net_state_dict'])
        self.target_net.load_state_dict(checkpoint['target_net_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

# Rainbow DQN Agent
class RainbowDQNAgent:
    """Rainbow DQN Agent (simplified, no multi-step for notebook)"""
    def __init__(self, policy_net, target_net, config):
        self.policy_net = policy_net
        self.target_net = target_net
        self.config = config
        
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()
        
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=config.LEARNING_RATE)
        self.device = torch.device(config.DEVICE if torch.cuda.is_available() else "cpu")
        self.policy_net.to(self.device)
        self.target_net.to(self.device)
        
        self.num_atoms = config.NUM_ATOMS if hasattr(config, 'NUM_ATOMS') else 51
        self.v_min = config.V_MIN if hasattr(config, 'V_MIN') else -10
        self.v_max = config.V_MAX if hasattr(config, 'V_MAX') else 10
        self.delta_z = (self.v_max - self.v_min) / (self.num_atoms - 1)
        self.support = torch.linspace(self.v_min, self.v_max, self.num_atoms).to(self.device)
        self.n_step_gamma = self.config.GAMMA ** (config.N_STEPS if hasattr(config, 'N_STEPS') else 3)
        
        # Cache offset tensor for categorical projection (batch_size is constant)
        batch_size = config.BATCH_SIZE if hasattr(config, 'BATCH_SIZE') else 32
        self.offset = torch.linspace(
            0, (batch_size - 1) * self.num_atoms, batch_size
        ).long().unsqueeze(1).expand(batch_size, self.num_atoms).to(self.device)
    
    def select_action(self, state, epsilon=None):
        """No epsilon needed - uses noisy networks for exploration"""
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device) / 255.0
            q_values = self.policy_net.get_q_values(state_tensor)
            return q_values.argmax(1).item()
    
    def get_max_q_value(self, state):
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device) / 255.0
            q_values = self.policy_net.get_q_values(state_tensor)
            return q_values.max().item()
    
    def train_step(self, replay_buffer):
        states, actions, rewards, next_states, dones, indices, weights = \
            replay_buffer.sample(self.config.BATCH_SIZE)
        
        # Convert uint8 to tensor on GPU, then normalize (faster than CPU normalization)
        states = torch.from_numpy(states).float().to(self.device) / 255.0
        actions = torch.LongTensor(actions).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        next_states = torch.from_numpy(next_states).float().to(self.device) / 255.0
        dones = torch.FloatTensor(dones).to(self.device)
        weights = torch.FloatTensor(weights).to(self.device)
        
        log_probs = self.policy_net(states)
        actions_expanded = actions.unsqueeze(1).unsqueeze(1).expand(-1, 1, self.num_atoms)
        log_probs = log_probs.gather(1, actions_expanded).squeeze(1)
        
        with torch.no_grad():
            next_q_values = self.policy_net.get_q_values(next_states)
            next_actions = next_q_values.argmax(1)
            
            next_log_probs = self.target_net(next_states)
            next_actions_expanded = next_actions.unsqueeze(1).unsqueeze(1).expand(-1, 1, self.num_atoms)
            next_log_probs = next_log_probs.gather(1, next_actions_expanded).squeeze(1)
            next_probs = next_log_probs.exp()
            
            target_distribution = self._categorical_projection(rewards, dones, next_probs)
        
        elementwise_loss = -(target_distribution * log_probs).sum(dim=1)
        loss = (elementwise_loss * weights).mean()
        
        with torch.no_grad():
            td_errors = (target_distribution * (target_distribution.log() - log_probs)).sum(dim=1).cpu().numpy()
        
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 10.0)
        self.optimizer.step()
        
        self.policy_net.reset_noise()
        self.target_net.reset_noise()
        
        replay_buffer.update_priorities(indices, td_errors)
        return loss.item()
    
    def _categorical_projection(self, rewards, dones, next_probs):
        batch_size = rewards.size(0)
        rewards = rewards.unsqueeze(1)
        dones = dones.unsqueeze(1)
        support = self.support.unsqueeze(0)
        
        # Using n-step gamma for Rainbow
        T_z = rewards + self.n_step_gamma * (1 - dones) * support
        T_z = T_z.clamp(self.v_min, self.v_max)
        
        b = (T_z - self.v_min) / self.delta_z
        l = b.floor().long()
        u = b.ceil().long()
        
        l[(u > 0) * (l == u)] -= 1
        u[(l < (self.num_atoms - 1)) * (l == u)] += 1
        
        target_distribution = torch.zeros_like(next_probs)
        
        # Use cached offset tensor (already on device)
        target_distribution.view(-1).index_add_(0, (l + self.offset).view(-1), (next_probs * (u.float() - b)).view(-1))
        target_distribution.view(-1).index_add_(0, (u + self.offset).view(-1), (next_probs * (b - l.float())).view(-1))
        
        return target_distribution
    
    def update_target_network(self):
        self.target_net.load_state_dict(self.policy_net.state_dict())
    
    def save(self, path):
        torch.save({
            'policy_net_state_dict': self.policy_net.state_dict(),
            'target_net_state_dict': self.target_net.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
        }, path)
    
    def load(self, path):
        checkpoint = torch.load(path)
        self.policy_net.load_state_dict(checkpoint['policy_net_state_dict'])
        self.target_net.load_state_dict(checkpoint['target_net_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

print("✓ Advanced components defined (Prioritized & Rainbow DQN)")
print(f"  Rainbow DQN: {sum(p.numel() for p in RainbowDQN().parameters()):,} params")


def evaluate_agent(agent, env_name, num_episodes=50):
    """Evaluate agent performance with epsilon=0 (exploitation only)"""
    eval_env = gym.make(env_name, frameskip=4)
    total_rewards = []
    
    for _ in range(num_episodes):
        obs, info = eval_env.reset()
        frame_stack = FrameStack(num_frames=FRAME_STACK)
        state = frame_stack.reset(obs)
        episode_reward = 0
        done = False
        
        while not done:
            # Check if agent requires epsilon (Rainbow doesn't)
            try:
                action = agent.select_action(state, epsilon=0.0)
            except TypeError:
                action = agent.select_action(state)
                
            next_obs, reward, terminated, truncated, info = eval_env.step(action)
            episode_reward += reward
            state = frame_stack.step(next_obs)
            done = terminated or truncated
            
        total_rewards.append(episode_reward)
    
    eval_env.close()
    return np.mean(total_rewards)

"""## 9. Training Function (Memory Optimized)"""

def train_dqn(algorithm_name, model_class, agent_class, num_episodes=1000):
    """
    Train DQN agent with memory optimizations

    Args:
        algorithm_name: "vanilla_dqn", "double_dqn", or "dueling_dqn"
        model_class: VanillaDQN, DoubleDQN, or DuelingDQN
        agent_class: VanillaDQNAgent, DoubleDQNAgent, or DuelingDQNAgent
        num_episodes: Number of episodes to train
    """
    log_dir = f"logs/{algorithm_name}"
    checkpoint_dir = f"checkpoints/{algorithm_name}"
    os.makedirs(log_dir, exist_ok=True)
    os.makedirs(checkpoint_dir, exist_ok=True)

    env = gym.make("ALE/Qbert-v5", frameskip=4)

    policy_net = model_class(input_channels=FRAME_STACK, num_actions=NUM_ACTIONS)
    target_net = model_class(input_channels=FRAME_STACK, num_actions=NUM_ACTIONS)

    # Create config object for agent initialization
    class Config:
        pass

    config = Config()
    config.LEARNING_RATE = LEARNING_RATE
    config.GAMMA = GAMMA
    config.BATCH_SIZE = BATCH_SIZE
    config.NUM_ACTIONS = NUM_ACTIONS
    config.DEVICE = DEVICE

    # Initialize agent with config object
    agent = agent_class(policy_net, target_net, config)

    replay_buffer = ReplayBuffer(capacity=REPLAY_BUFFER_SIZE)
    logger = DQNLogger(log_dir, algorithm_name)
    logger.save_config({
        'algorithm': algorithm_name,
        'learning_rate': LEARNING_RATE,
        'gamma': GAMMA,
        'batch_size': BATCH_SIZE,
        'epsilon_start': EPSILON_START,
        'epsilon_end': EPSILON_END,
        'epsilon_decay_steps': EPSILON_DECAY_STEPS,
        'replay_buffer_size': REPLAY_BUFFER_SIZE,
        'target_update': TARGET_UPDATE,
    })

    global_step = 0
    epsilon = EPSILON_START
    epsilon_decay = (EPSILON_START - EPSILON_END) / EPSILON_DECAY_STEPS

    # Track saved checkpoints for cleanup (keep only 2 most recent)
    saved_checkpoints = []

    print(f"\n{'='*60}")
    print(f"Training {algorithm_name.upper()}")
    print(f"{'='*60}")

    for episode in tqdm(range(num_episodes), desc=f"Training {algorithm_name}"):
        episode_start_time = time.time()

        obs, info = env.reset()
        frame_stack = FrameStack(num_frames=FRAME_STACK)
        state = frame_stack.reset(obs)

        episode_reward = 0
        episode_length = 0
        # Use total_reward for benchmarking instead
        for step in range(MAX_STEPS_PER_EPISODE):
            action = agent.select_action(state, epsilon)
            next_obs, reward, terminated, truncated, info = env.step(action)
            
            # Use reward scaling (0.01) instead of hard clipping
            # This preserves difference between small and large rewards while keeping gradients stable
            scaled_reward = reward * 0.01
            
            done = terminated or truncated
            next_state = frame_stack.step(next_obs)

            replay_buffer.push(state, action, scaled_reward, next_state, done)

            loss = None
            if len(replay_buffer) > MIN_REPLAY_SIZE and global_step % UPDATE_FREQUENCY == 0:
                # FIXED: Don't pass BATCH_SIZE - agent gets it from config
                loss = agent.train_step(replay_buffer)

            if global_step % TARGET_UPDATE == 0:
                agent.update_target_network()

            if global_step % STEP_LOG_FREQUENCY == 0:
                q_value = agent.get_max_q_value(state)
                logger.log_step(global_step, episode, step, action, reward, loss, q_value, epsilon)

            state = next_state
            episode_reward += reward # Track unscaled reward for logging
            episode_length += 1
            global_step += 1
            epsilon = max(EPSILON_END, epsilon - epsilon_decay)

            if done:
                break

        episode_time = time.time() - episode_start_time
        buffer_stats = replay_buffer.get_reward_stats()

        # Run evaluation every 50 episodes
        eval_reward = None
        if (episode + 1) % 200 == 0:
            eval_reward = evaluate_agent(agent, "ALE/Qbert-v5")
            print(f"  Eval Reward (eps 0.0): {eval_reward:.2f}")

        logger.log_episode(episode, episode_reward, episode_length, epsilon, global_step,
                          episode_time, buffer_stats, eval_reward)

        # MEMORY OPTIMIZATION: Periodic garbage collection
        if (episode + 1) % GC_FREQUENCY == 0:
            gc.collect()
            if DEVICE == "cuda":
                torch.cuda.empty_cache()

        # Save checkpoint and keep only 2 most recent
        if (episode + 1) % SAVE_FREQUENCY == 0:
            checkpoint_path = os.path.join(checkpoint_dir, f"ep_{episode+1}.pth")
            torch.save({
                'episode': episode,
                'policy_net_state_dict': policy_net.state_dict(),
                'target_net_state_dict': target_net.state_dict(),
                'optimizer_state_dict': agent.optimizer.state_dict(),
                'epsilon': epsilon,
                'global_step': global_step,
            }, checkpoint_path)

            saved_checkpoints.append(checkpoint_path)

            # Keep only 2 most recent checkpoints
            if len(saved_checkpoints) > 2:
                old_checkpoint = saved_checkpoints.pop(0)
                if os.path.exists(old_checkpoint):
                    os.remove(old_checkpoint)
                    print(f"  Removed old checkpoint: {os.path.basename(old_checkpoint)}")

        if (episode + 1) % 100 == 0:
            print(f"\nEp {episode+1}/{num_episodes} | Reward: {episode_reward:.0f} | "
                  f"Steps: {episode_length} | Eps: {epsilon:.3f} | Time: {episode_time:.1f}s")
            if DEVICE == "cuda":
                print(f"  GPU Memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB")

    # Save final checkpoint (always keep this one)
    final_path = os.path.join(checkpoint_dir, "final.pth")
    torch.save({
        'episode': num_episodes,
        'policy_net_state_dict': policy_net.state_dict(),
        'target_net_state_dict': target_net.state_dict(),
        'optimizer_state_dict': agent.optimizer.state_dict(),
        'epsilon': epsilon,
        'global_step': global_step,
    }, final_path)

    logger.close()
    env.close()

    print(f"\n✓ {algorithm_name} training complete!")
    print(f"  Logs: {log_dir}")
    print(f"  Checkpoints: {checkpoint_dir}")
    print(f"  Final checkpoint: {final_path}")
print("✓ Training function defined")

"""## 10. Train Vanilla DQN"""

train_dqn("vanilla_dqn", VanillaDQN, VanillaDQNAgent, num_episodes=TOTAL_EPISODES)

# Clear memory between algorithms
gc.collect()
if DEVICE == "cuda":
    torch.cuda.empty_cache()
print("\n[Memory cleared after Vanilla DQN]\n")

"""## 11. Train Double DQN"""

train_dqn("double_dqn", DoubleDQN, DoubleDQNAgent, num_episodes=TOTAL_EPISODES)

# Clear memory between algorithms
gc.collect()
if DEVICE == "cuda":
    torch.cuda.empty_cache()
print("\n[Memory cleared after Double DQN]\n")

"""## 12. Train Dueling DQN"""

train_dqn("dueling_dqn", DuelingDQN, DuelingDQNAgent, num_episodes=TOTAL_EPISODES)

# Clear memory between algorithms
gc.collect()
if DEVICE == "cuda":
    torch.cuda.empty_cache()
print("\n[Memory cleared after Dueling DQN]\n")

"""## 13. Train Prioritized DQN"""

def train_prioritized_dqn(algorithm_name, model_class, agent_class, num_episodes=1000):
    """Train with Prioritized Replay Buffer"""
    log_dir = f"logs/{algorithm_name}"
    checkpoint_dir = f"checkpoints/{algorithm_name}"
    os.makedirs(log_dir, exist_ok=True)
    os.makedirs(checkpoint_dir, exist_ok=True)

    env = gym.make("ALE/Qbert-v5", frameskip=4)

    policy_net = model_class(input_channels=FRAME_STACK, num_actions=NUM_ACTIONS)
    target_net = model_class(input_channels=FRAME_STACK, num_actions=NUM_ACTIONS)

    class Config:
        pass

    config = Config()
    config.LEARNING_RATE = LEARNING_RATE
    config.GAMMA = GAMMA
    config.BATCH_SIZE = BATCH_SIZE
    config.NUM_ACTIONS = NUM_ACTIONS
    config.DEVICE = DEVICE

    agent = agent_class(policy_net, target_net, config)
    replay_buffer = PrioritizedReplayBuffer(capacity=REPLAY_BUFFER_SIZE, alpha=0.6, beta_start=0.4, beta_frames=100000)
    
    logger = DQNLogger(log_dir, algorithm_name)
    logger.save_config({
        'algorithm': algorithm_name,
        'learning_rate': LEARNING_RATE,
        'gamma': GAMMA,
        'batch_size': BATCH_SIZE,
        'epsilon_start': EPSILON_START,
        'epsilon_end': EPSILON_END,
        'epsilon_decay_steps': EPSILON_DECAY_STEPS,
        'replay_buffer_size': REPLAY_BUFFER_SIZE,
        'target_update': TARGET_UPDATE,  
        'alpha': 0.6,
        'beta_start': 0.4,
    })

    global_step = 0
    epsilon = EPSILON_START
    epsilon_decay = (EPSILON_START - EPSILON_END) / EPSILON_DECAY_STEPS
    saved_checkpoints = []

    print(f"\n{'='*60}")
    print(f"Training {algorithm_name.upper()}")
    print(f"{'='*60}")

    for episode in tqdm(range(num_episodes), desc=f"Training {algorithm_name}"):
        episode_start_time = time.time()

        obs, info = env.reset()
        frame_stack = FrameStack(num_frames=FRAME_STACK)
        state = frame_stack.reset(obs)

        episode_reward = 0
        episode_length = 0

        for step in range(MAX_STEPS_PER_EPISODE):
            action = agent.select_action(state, epsilon)
            next_obs, reward, terminated, truncated, info = env.step(action)
            
            scaled_reward = reward * 0.01
            done = terminated or truncated
            next_state = frame_stack.step(next_obs)

            replay_buffer.push(state, action, scaled_reward, next_state, done)

            loss = None
            if len(replay_buffer) > MIN_REPLAY_SIZE and global_step % UPDATE_FREQUENCY == 0:
                loss = agent.train_step(replay_buffer)

            if global_step % TARGET_UPDATE == 0:
                agent.update_target_network()

            if global_step % STEP_LOG_FREQUENCY == 0:
                q_value = agent.get_max_q_value(state)
                logger.log_step(global_step, episode, step, action, reward, loss, q_value, epsilon)

            state = next_state
            episode_reward += reward
            episode_length += 1
            global_step += 1
            epsilon = max(EPSILON_END, epsilon - epsilon_decay)

            if done:
                break

        episode_time = time.time() - episode_start_time
        buffer_stats = replay_buffer.get_reward_stats()
        
        # Run evaluation every 200 episodes
        eval_reward = None
        if (episode + 1) % 200 == 0:
            eval_reward = evaluate_agent(agent, "ALE/Qbert-v5")
            print(f"  Eval Reward (eps 0.0): {eval_reward:.2f}")

        logger.log_episode(episode, episode_reward, episode_length, epsilon, global_step, episode_time, buffer_stats, eval_reward)

        if (episode + 1) % GC_FREQUENCY == 0:
            gc.collect()
            if DEVICE == "cuda":
                torch.cuda.empty_cache()

        if (episode + 1) % SAVE_FREQUENCY == 0:
            checkpoint_path = os.path.join(checkpoint_dir, f"ep_{episode+1}.pth")
            torch.save({'episode': episode, 'policy_net_state_dict': policy_net.state_dict(),
                       'target_net_state_dict': target_net.state_dict(),
                       'optimizer_state_dict': agent.optimizer.state_dict()}, checkpoint_path)
            saved_checkpoints.append(checkpoint_path)
            if len(saved_checkpoints) > 2:
                old_checkpoint = saved_checkpoints.pop(0)
                if os.path.exists(old_checkpoint):
                    os.remove(old_checkpoint)

        if (episode + 1) % 100 == 0:
            print(f"\nEp {episode+1}/{num_episodes} | Reward: {episode_reward:.0f} | Steps: {episode_length} | Eps: {epsilon:.3f}")

    final_path = os.path.join(checkpoint_dir, "final.pth")
    torch.save({'policy_net_state_dict': policy_net.state_dict()}, final_path)
    logger.close()
    env.close()
    print(f"\n✓ {algorithm_name} training complete!")

train_prioritized_dqn("prioritized_dqn", VanillaDQN, PrioritizedDQNAgent, num_episodes=TOTAL_EPISODES)

# Clear memory between algorithms
gc.collect()
if DEVICE == "cuda":
    torch.cuda.empty_cache()
print("\n[Memory cleared after Prioritized DQN]\n")

"""## 14. Train Rainbow DQN"""

def train_rainbow_dqn(num_episodes=1000):
    """Train Rainbow DQN"""
    algorithm_name = "rainbow_dqn"
    log_dir = f"logs/{algorithm_name}"
    checkpoint_dir = f"checkpoints/{algorithm_name}"
    os.makedirs(log_dir, exist_ok=True)
    os.makedirs(checkpoint_dir, exist_ok=True)

    env = gym.make("ALE/Qbert-v5", frameskip=4)

    # MEMORY OPTIMIZED: Reduce atoms from 51 to 21 for notebook training
    policy_net = RainbowDQN(input_channels=FRAME_STACK, num_actions=NUM_ACTIONS, num_atoms=21, v_min=-10, v_max=10)
    target_net = RainbowDQN(input_channels=FRAME_STACK, num_actions=NUM_ACTIONS, num_atoms=21, v_min=-10, v_max=10)

    class Config:
        pass

    config = Config()
    config.LEARNING_RATE = LEARNING_RATE
    config.GAMMA = GAMMA
    config.BATCH_SIZE = BATCH_SIZE  # Same as other algorithms (32)
    config.NUM_ACTIONS = NUM_ACTIONS
    config.DEVICE = DEVICE
    config.NUM_ATOMS = 21  # MEMORY OPTIMIZED: 21 instead of 51 to avoid OOM
    config.V_MIN = -10
    config.V_MAX = 10
    config.N_STEPS = 3

    agent = RainbowDQNAgent(policy_net, target_net, config)

    # Use same buffer size as other algorithms for fair comparison
    replay_buffer = PrioritizedReplayBuffer(capacity=REPLAY_BUFFER_SIZE, alpha=0.6, beta_start=0.4, beta_frames=100000)
    n_step_buffer = MultiStepBuffer(n_steps=config.N_STEPS, gamma=GAMMA)

    logger = DQNLogger(log_dir, algorithm_name)
    logger.save_config({
        'algorithm': 'Rainbow DQN',
        'learning_rate': LEARNING_RATE,
        'gamma': GAMMA,
        'batch_size': BATCH_SIZE,
        'replay_buffer_size': REPLAY_BUFFER_SIZE,
        'target_update': TARGET_UPDATE,
        'num_atoms': config.NUM_ATOMS,
        'n_steps': config.N_STEPS,
    })

    global_step = 0
    saved_checkpoints = []

    print(f"\n{'='*60}")
    print(f"Training RAINBOW DQN")
    print(f"  Note: Using {config.NUM_ATOMS} atoms (vs 51 standard) to fit in notebook memory")
    print(f"  Buffer: {REPLAY_BUFFER_SIZE:,}, Batch: {BATCH_SIZE}, N-step: {config.N_STEPS}")
    print(f"{'='*60}")

    for episode in tqdm(range(num_episodes), desc="Training Rainbow DQN"):
        episode_start_time = time.time()

        obs, info = env.reset()
        frame_stack = FrameStack(num_frames=FRAME_STACK)
        state = frame_stack.reset(obs)

        episode_reward = 0
        episode_length = 0
        n_step_buffer.reset()

        for step in range(MAX_STEPS_PER_EPISODE):
            action = agent.select_action(state)  # No epsilon!
            next_obs, reward, terminated, truncated, info = env.step(action)
            
            scaled_reward = reward * 0.01
            done = terminated or truncated
            next_state = frame_stack.step(next_obs)

            # Store in multi-step buffer instead of directly in replay buffer
            n_step_buffer.append(state, action, scaled_reward, next_state, done)
            if len(n_step_buffer) == config.N_STEPS:
                transition = n_step_buffer.get()
                if transition is not None:
                    replay_buffer.push(*transition)

            loss = None
            if len(replay_buffer) > MIN_REPLAY_SIZE and global_step % UPDATE_FREQUENCY == 0:
                loss = agent.train_step(replay_buffer)

            if global_step % TARGET_UPDATE == 0:
                agent.update_target_network()

            if global_step % STEP_LOG_FREQUENCY == 0:
                q_value = agent.get_max_q_value(state)
                logger.log_step(global_step, episode, step, action, reward, loss, q_value, 0.0)  # epsilon=0

            state = next_state
            episode_reward += reward
            episode_length += 1
            global_step += 1

            if done:
                # Flush remaining transitions in n-step buffer
                while len(n_step_buffer) > 0:
                    transition = n_step_buffer.get(force=True)
                    if transition is not None:
                        replay_buffer.push(*transition)
                    # Remove from front to avoid keeping stale references
                    if len(n_step_buffer.buffer) > 0:
                        n_step_buffer.buffer.popleft()
                # Final cleanup with garbage collection
                n_step_buffer.reset()
                break

        episode_time = time.time() - episode_start_time
        buffer_stats = replay_buffer.get_reward_stats()
        
        # Run evaluation every 200 episodes
        eval_reward = None
        if (episode + 1) % 200 == 0:
            eval_reward = evaluate_agent(agent, "ALE/Qbert-v5")
            print(f"  Eval Reward (eps 0.0): {eval_reward:.2f}")

        logger.log_episode(episode, episode_reward, episode_length, 0.0, global_step, episode_time, buffer_stats, eval_reward)

        # AGGRESSIVE MEMORY MANAGEMENT for Rainbow (every 10 episodes)
        if (episode + 1) % 10 == 0:
            gc.collect()
            if DEVICE == "cuda":
                torch.cuda.empty_cache()

        if (episode + 1) % GC_FREQUENCY == 0:
            gc.collect()
            if DEVICE == "cuda":
                torch.cuda.empty_cache()

        if (episode + 1) % SAVE_FREQUENCY == 0:
            checkpoint_path = os.path.join(checkpoint_dir, f"ep_{episode+1}.pth")
            torch.save({'episode': episode, 'policy_net_state_dict': policy_net.state_dict()}, checkpoint_path)
            saved_checkpoints.append(checkpoint_path)
            if len(saved_checkpoints) > 2:
                old_checkpoint = saved_checkpoints.pop(0)
                if os.path.exists(old_checkpoint):
                    os.remove(old_checkpoint)

        if (episode + 1) % 100 == 0:
            print(f"\nEp {episode+1}/{num_episodes} | Reward: {episode_reward:.0f} | Steps: {episode_length}")

    final_path = os.path.join(checkpoint_dir, "final.pth")
    torch.save({'policy_net_state_dict': policy_net.state_dict()}, final_path)
    logger.close()
    env.close()
    print(f"\n✓ Rainbow DQN training complete!")

train_rainbow_dqn(num_episodes=TOTAL_EPISODES)

"""## 15. Download Logs

Download the logs for visualization on your local machine.
"""

# Zip logs for download
!zip -r dqn_logs.zip logs/
!zip -r dqn_checkpoints.zip checkpoints/

print("✓ Logs and checkpoints zipped")
print("  Download: dqn_logs.zip, dqn_checkpoints.zip")

"""## 16. Quick Analysis"""

import pandas as pd

# Load logs
vanilla_df = pd.read_csv("logs/vanilla_dqn/training_log.csv")
double_df = pd.read_csv("logs/double_dqn/training_log.csv")
dueling_df = pd.read_csv("logs/dueling_dqn/training_log.csv")
prioritized_df = pd.read_csv("logs/prioritized_dqn/training_log.csv")
rainbow_df = pd.read_csv("logs/rainbow_dqn/training_log.csv")

# Print summary
print("="*60)
print("TRAINING SUMMARY - ALL 5 DQN VARIANTS")
print("="*60)

for name, df in [("Vanilla DQN", vanilla_df), ("Double DQN", double_df), ("Dueling DQN", dueling_df),
                 ("Prioritized DQN", prioritized_df), ("Rainbow DQN", rainbow_df)]:
    print(f"\n{name}:")
    print(f"  Average Reward: {df['total_reward'].mean():.2f} ± {df['total_reward'].std():.2f}")
    print(f"  Max Reward: {df['total_reward'].max():.2f}")
    print(f"  Average Training Time: {df['training_time_seconds'].mean():.2f}s per episode")
    print(f"  Total Training Time: {df['training_time_seconds'].sum() / 3600:.2f} hours")
    

print("\n" + "="*60)